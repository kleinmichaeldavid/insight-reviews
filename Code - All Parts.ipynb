{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "- preprocessing\n",
    "- predictions\n",
    "- creating visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Needed for Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(review_df, sentiment_cutoff = 4,\n",
    "                   text_col = 'review_text', rating_col = 'rating'):\n",
    "    \n",
    "    ## drop rows that don't contain any review text\n",
    "    review_df = review_df[~review_df[text_col].isnull()].reset_index(drop=True)\n",
    "    \n",
    "    ## drop rows with improper rating\n",
    "    review_df = review_df[review_df[rating_col].isin([5,4,3,2,1,'5','4','3','2','1'])]\n",
    "    \n",
    "    ## convert ratings to numeric (in case some are str)\n",
    "    review_df[rating_col]= review_df[rating_col].astype(int) \n",
    "    \n",
    "    ## add y column for sentiment classification. ratings > x are positive, <= are negative\n",
    "    review_df['y'] = (review_df[rating_col] > sentiment_cutoff)\n",
    "    \n",
    "    ## remove unneccessary columns\n",
    "    review_df = review_df.loc[:,['review_text','y']]\n",
    "    \n",
    "    return review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed for Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import stop_words as sw_sklearn\n",
    "from nltk.corpus import stopwords as sw_nltk\n",
    "from gensim.parsing.preprocessing import STOPWORDS as sw_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_splitter(reviews, split_on):\n",
    "    sentences = [re.split(split_on,rev) for rev in reviews]\n",
    "    sentences = [item.strip() for sublist in sentences for item in sublist]\n",
    "    sentences = [sentence for sentence in sentences if sentence != '']\n",
    "    return sentences\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def number_replacer(sentence):\n",
    "    return re.sub('[A-Za-z]*[0-9]+[A-Za-z]*', 'numbertoken', sentence)\n",
    "\n",
    "def tokenize_text(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    return sentence\n",
    "\n",
    "def alphanumerize(sentence, remove_pattern):\n",
    "    removed = re.sub('<br>', '', sentence)\n",
    "    removed = re.sub('&amp', '', removed) ## since the word 'amp' keeps showing up\n",
    "    removed = re.sub('[Aa]\\+', 'amazing', removed) ## A+ --> amazing\n",
    "    removed = re.sub('thank you', 'thanks', removed) ## maybe remove this\n",
    "    removed = re.sub(remove_pattern, '', removed)\n",
    "    return removed\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n",
    "\n",
    "def remove_stopwords(sentence, stopwords):\n",
    "    return ' '.join([word for word in sentence.split(' ') if word not in stopwords])\n",
    "\n",
    "def get_w2v_features(w2v_model, sentence):\n",
    "    \n",
    "    index2word_set = set(w2v_model.wv.vocab.keys()) # create vocab set\n",
    "    embedding = np.zeros(w2v_model.vector_size, dtype=\"float32\") # init vector\n",
    "    \n",
    "    ## there shouldn't be empty sentences, but include this just in case\n",
    "    if len(sentence) > 0:\n",
    "        nwords = 0 # word counter\n",
    "        for word in sentence:\n",
    "            if word in index2word_set:\n",
    "                # if the word is in the set, add it and increment counter\n",
    "                embedding = np.add(embedding, w2v_model[word])\n",
    "                nwords += 1.\n",
    "        if nwords > 0: # in case no words were aded\n",
    "            embedding = np.divide(embedding, nwords)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(review_df,\n",
    "                  split_sentences = False,\n",
    "                  replace_numbers = True,\n",
    "                  tokenize = True,\n",
    "                  lemmatize = True,\n",
    "                  remove_stops = True,\n",
    "                  embed = True,\n",
    "                  output_embeddings = True,\n",
    "                  split_on = \"[\\.!]+[ ]*\",\n",
    "                  stopwords = '',\n",
    "                  remove_pattern = '[^0-9A-Za-z ]+',\n",
    "                  W2Vmodel = '',\n",
    "                  old_col='review_text',\n",
    "                  new_col='clean_text'):\n",
    "    \n",
    "    ## other options...\n",
    "    # replace rare words with a certain token\n",
    "    \n",
    "    ## split into sentences (optional)\n",
    "    if split_sentences:\n",
    "        print('splitting into sentences...')\n",
    "        review_df = pd.DataFrame({old_col : sentence_splitter(review_df[old_col],split_on)})\n",
    "    \n",
    "    ## remove accents\n",
    "    print('removing accents...')\n",
    "    review_df[new_col] = list(map(remove_accented_chars, review_df[old_col]))\n",
    "    \n",
    "    ## convert all text to lowercase\n",
    "    print('converting to lowercase...')\n",
    "    review_df[new_col] = review_df[new_col].str.lower()\n",
    "    \n",
    "    ## expand contractions\n",
    "    print('expanding contractions...')\n",
    "    review_df[new_col] = list(map(expand_contractions,review_df[new_col]))\n",
    "    \n",
    "    ## replace numbers (optional)\n",
    "    if replace_numbers:\n",
    "        print('replacing numbers...')\n",
    "        review_df[new_col] = list(map(number_replacer,review_df[new_col]))\n",
    "    \n",
    "    ## clear non-alphanumerics\n",
    "    print('alphanumerizing...')\n",
    "    review_df[new_col] = [alphanumerize(rev, remove_pattern) for rev in review_df[new_col]]\n",
    "    \n",
    "    ## lemmatize (optional)\n",
    "    if lemmatize:\n",
    "        print('lemmatizing...')\n",
    "        review_df[new_col] = list(map(lemmatize_text,review_df[new_col]))\n",
    "        \n",
    "    ## remove stop words (optional)\n",
    "    if remove_stops:\n",
    "        print('removing stop words...')\n",
    "        if stopwords == '':\n",
    "            stopwords = nltk.corpus.stopwords.words('english')\n",
    "            stopwords.remove('not')\n",
    "        if lemmatize:\n",
    "            stopwords = [lemmatizer.lemmatize(w) for w in stopwords]\n",
    "        review_df[new_col] = [remove_stopwords(rev, stopwords) for rev in review_df[new_col]]\n",
    "\n",
    "    ## tokenize (optional)\n",
    "    if tokenize:\n",
    "        print('tokenizing...')\n",
    "        review_df[new_col] = list(map(tokenize_text,review_df[new_col]))\n",
    "    \n",
    "    ## embed (optional)\n",
    "    if embed:\n",
    "        print('embedding words...')\n",
    "        review_df['embeddings'] = list(map(lambda sentence:get_w2v_features(W2Vmodel, sentence),\n",
    "                                    review_df[new_col]))\n",
    "        \n",
    "        if output_embeddings:\n",
    "            print('converting to embedded form...')\n",
    "            review_df = review_df['embeddings'].apply(pd.Series)\n",
    "    \n",
    "    return review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stopwords(my_stopwords):\n",
    "    stopwords_sklearn = list(sw_sklearn.ENGLISH_STOP_WORDS)\n",
    "    stopwords_nltk = list(sw_nltk.words('english'))\n",
    "    stopwords_gensim = list(sw_gensim)\n",
    "\n",
    "    custom_stopwords = set(stopwords_sklearn +\n",
    "                           stopwords_nltk +\n",
    "                           stopwords_gensim +\n",
    "                           my_stopwords)\n",
    "    \n",
    "    return custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_test(rev, pool, vecs):\n",
    "    similarity = 0\n",
    "    for rev_word in rev:\n",
    "        if rev_word in vecs:\n",
    "            for pool_word in pool:\n",
    "                if pool_word in vecs:\n",
    "                    word_sim = vecs.similarity(rev_word,pool_word)\n",
    "                    similarity = np.max([word_sim,similarity])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_contribution(similarity):\n",
    "    return round(np.min([np.max([similarity * 2 - 1, 0]),1]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup (not part of web app)\n",
    "- eda\n",
    "- building / saving a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('review_data_200_sellers.csv')\n",
    "#data = pd.read_csv('review_data_2800_sellers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = basic_cleaning(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up initial W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n",
      "tokenizing...\n"
     ]
    }
   ],
   "source": [
    "## process data to put into w2v model\n",
    "data_for_w2v = preprocessing(data,\n",
    "                             split_sentences=False,\n",
    "                             remove_stops=False,\n",
    "                             embed = False,\n",
    "                             stopwords='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.7 s, sys: 30.5 ms, total: 42.7 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### set up and train the w2v model\n",
    "### I think this is a spot that could be improved...\n",
    "# how does it decide which words to include? is it starting from pretrained?\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality\n",
    "min_word_count = 10   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "W2Vmodel = Word2Vec(sentences=data_for_w2v['clean_text'],\n",
    "                    sg=1,\n",
    "                    hs=0,\n",
    "                    workers=num_workers,\n",
    "                    size=num_features,\n",
    "                    min_count=min_word_count,\n",
    "                    window=context,\n",
    "                    sample=downsampling,\n",
    "                    negative=5,\n",
    "                    iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this was an attempt to initialize from pretrained embeddings\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = 'glove.twitter.27B.50d.txt'\n",
    "# word2vec_output_file = 'word2vec.twitter.27B.50d.txt'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# from gensim.models import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format(\"word2vec.twitter.27B.50d.txt\", binary=False)\n",
    "# model_2 = Word2Vec(size=300, min_count=1)\n",
    "# model_2.build_vocab(sentences=data_for_w2v['clean_text'])\n",
    "# total_examples = model_2.corpus_count\n",
    "# model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "# model_2.intersect_word2vec_format(\"word2vec.twitter.27B.50d.txt\", binary=False, lockf=1.0)\n",
    "# model_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create cluster model to identify non-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n",
      "tokenizing...\n",
      "embedding words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/insight/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to embedded form...\n"
     ]
    }
   ],
   "source": [
    "## process the data for the language model\n",
    "data_for_language = preprocessing(data,\n",
    "                                  split_sentences = False,\n",
    "                                  replace_numbers = True,\n",
    "                                  tokenize = True,\n",
    "                                  lemmatize = True,\n",
    "                                  remove_stops = False,\n",
    "                                  embed = True,\n",
    "                                  output_embeddings = True,\n",
    "                                  W2Vmodel = W2Vmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the clustering model on 2 clusters (english vs non-english)\n",
    "#kmeans_vecs_2 = KMeans(n_clusters=2, max_iter=600, algorithm = 'auto')\n",
    "#kmeans_vecs_2.fit(data_for_language)\n",
    "\n",
    "## dump the model\n",
    "#joblib.dump(kmeans_vecs_2, 'kmeans_for_language.pkl')\n",
    "kmeans_vecs_2 = open('kmeans_for_language.pkl','rb')\n",
    "kmeans_vecs_2 = joblib.load(kmeans_vecs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: review_text, dtype: object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## confirm that it worked & identify the \n",
    "data_transformed = kmeans_vecs_2.predict(data_for_language)\n",
    "data.review_text[data_transformed == 0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an english-only dataset\n",
    "data_english = data[data_transformed == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain the W2V Model on english only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n",
      "tokenizing...\n"
     ]
    }
   ],
   "source": [
    "## process data to put into w2v model\n",
    "data_for_w2v = preprocessing(data,\n",
    "                             split_sentences=False,\n",
    "                             remove_stops=False,\n",
    "                             embed = False,\n",
    "                             stopwords='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "# W2Vmodel = Word2Vec(sentences=data_for_w2v['clean_text'],\n",
    "#                     sg=1,\n",
    "#                     hs=0,\n",
    "#                     workers=num_workers,\n",
    "#                     size=num_features,\n",
    "#                     min_count=min_word_count,\n",
    "#                     window=context,\n",
    "#                     sample=downsampling,\n",
    "#                     negative=5,\n",
    "#                     iter=6)\n",
    "\n",
    "#joblib.dump(kmeans_vecs_2, 'kmeans_for_language.pkl')\n",
    "W2Vmodel = open('W2Vmodel.pkl','rb')\n",
    "W2Vmodel = joblib.load(W2Vmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Clustering (or, just some code to measure similarity?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write this to another file.\n",
    "stopwords_mike = ['bracelets','necklace','earrings',\n",
    "                 'bracelet','ring','daughter','mother',\n",
    "                 'anklet','cindy','gift','gorgeous',\n",
    "                 'item','product','color','colors',\n",
    "                 'lovely','perfect','cute','beautiful','great',\n",
    "                  'super','order','wonderful','work','awesome',\n",
    "                 'loved','lt','pretty','little','love','absolutely',\n",
    "                 'bunch','wear','friend','piece','amazing','happy',\n",
    "                  'fantastic','xx','summer','excellent','million',\n",
    "                 'michael','lisa','good','looks','highly','recommend',\n",
    "                 'price','leather','future','pleased','high','beads',\n",
    "                  'you','nice','size','looking',\n",
    "                 'design','buy','definitely','fast','quickly',\n",
    "                 'products','product','promptly','quickly','quick',\n",
    "                 'numbertoken','like','ordered','fit','purchase','bought',\n",
    "                  'christmas','beautifully','perfectly','purchased','husband',\n",
    "                  'ordering','adorable','way','small','better','wrist',\n",
    "                  'jewelry','wearing','day','wanted', 'easy', 'new','lot',\n",
    "                  'best','going', 'custom','sure', 'simple','right','person',\n",
    "                  'boyfriend','took', 'charm', 'sent', 'photo', 'week','use',\n",
    "                  'birthday','son','extra','bit','favorite', 'clasp','present',\n",
    "                  'nose','bag','sister','chain','mom', 'year', 'stone','wedding',\n",
    "                  'thought','delicate','feel','gave', 'different','dainty',\n",
    "                  'box','big','second','blanket','sweet','comfortable',\n",
    "                  'thing','extremely','baby','silver','want']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = create_stopwords(stopwords_mike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting into sentences...\n",
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n",
      "removing stop words...\n",
      "tokenizing...\n",
      "embedding words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/insight/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to embedded form...\n"
     ]
    }
   ],
   "source": [
    "data_for_clustering = preprocessing(data_english,\n",
    "                                    split_sentences=True,\n",
    "                                    embed=True,\n",
    "                                    output_embeddings=True,\n",
    "                                    W2Vmodel = W2Vmodel,\n",
    "                                    stopwords=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clustering goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-up Word Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put this into a separate file\n",
    "\n",
    "uniqueness_pool = ['unique', 'creative', 'special',\n",
    "                   'inventive', 'innovative', 'handmade',\n",
    "                   'original', 'handcrafted',\n",
    "                   'creativity', 'crafted','made']\n",
    "\n",
    "humantouch_pool = ['helpful', 'communication', 'response',\n",
    "                   'respond', 'contact', 'friendly',\n",
    "                   'attentive', 'accommodating', 'courteous',\n",
    "                   'polite', 'respect', 'pleasant',\n",
    "                   'service','kind','personal','seller',\n",
    "                   'personable','interaction','reply','answer',\n",
    "                   'honest']\n",
    "# this will hit on other types of interaction\n",
    "# future - different types of human touch (e.g. responsiveness, personalization, etc)\n",
    "# need to train my word vectors on more negative reviews (use pre-trained vectors to get negative words)\n",
    "# also, problem of having positive words in my topics\n",
    "\n",
    "quality_pool = ['quality', 'condition', 'described',\n",
    "                'detailed', 'craftmanship', 'craftsmanship',\n",
    "                'detail', 'workmanship', 'material']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ['service','trustworthy','honest','seller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_contribution(similarity_test(review,\n",
    "                                   humantouch_pool,\n",
    "                                   W2Vmodel.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this was an abandoned way to classify the topic\n",
    "\n",
    "# uniqueness_vector = np.zeros(200)\n",
    "# n_words = 0\n",
    "# for word in uniqueness_pool:\n",
    "#     if word in word_vectors:\n",
    "#         n_words += 1\n",
    "#         uniqueness_vector += word_vectors[word]\n",
    "# uniqueness_vector = uniqueness_vector / n_words\n",
    "# print('words: ' + str(n_words))\n",
    "# word_vectors.most_similar([uniqueness_vector])\n",
    "\n",
    "# humantouch_vector = np.zeros(200)\n",
    "# n_words = 0\n",
    "# for word in humantouch_pool:\n",
    "#     if word in word_vectors:\n",
    "#         n_words += 1\n",
    "#         humantouch_vector += word_vectors[word]\n",
    "# humantouch_vector = humantouch_vector / n_words\n",
    "# print('words: ' + str(n_words))\n",
    "# word_vectors.most_similar([humantouch_vector])\n",
    "\n",
    "# quality_vector = np.zeros(200)\n",
    "# n_words = 0\n",
    "# for word in quality_pool:\n",
    "#     if word in word_vectors:\n",
    "#         n_words += 1\n",
    "#         quality_vector += word_vectors[word]\n",
    "# quality_vector = quality_vector / n_words\n",
    "# print('words: ' + str(n_words))\n",
    "# word_vectors.most_similar([quality_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler # why this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n"
     ]
    }
   ],
   "source": [
    "## process the data for the sentiment model\n",
    "data_for_sentiment = preprocessing(data,\n",
    "                                   split_sentences = False,\n",
    "                                   replace_numbers = True,\n",
    "                                   tokenize = False,\n",
    "                                   lemmatize = True,\n",
    "                                   remove_stops = False,\n",
    "                                   embed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### but for the actual model I want to train this on thentire dataset!\n",
    "## training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_for_sentiment['clean_text'],\n",
    "                                                    data_for_sentiment['y'],\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=20313119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train.to_frame(), y_train)\n",
    "X_resampled = pd.Series(X_resampled[:,0]); y_resampled = pd.Series(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = make_pipeline(TfidfVectorizer(ngram_range = (1,2)),\n",
    "                         MultinomialNB())\n",
    "\n",
    "#model_nb.fit(X_resampled, y_resampled)\n",
    "#joblib.dump(model_nb, 'model_nb_200sellers.pkl')\n",
    "model_nb = open('model_nb_200sellers.pkl','rb')\n",
    "model_nb = joblib.load(model_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = model_nb.predict(X_train)\n",
    "yhat_test = model_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9026732216930404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>239</td>\n",
       "      <td>1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2187</td>\n",
       "      <td>11751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True \n",
       "Actual                 \n",
       "False        239   1267\n",
       "True        2187  11751"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion = pd.crosstab(pd.Series(y_test.reset_index(drop=True),name='Actual'),\n",
    "                           pd.Series(yhat_test,name='Predicted'))\n",
    "precision_test = df_confusion.loc[True,True] / np.sum(df_confusion.loc[:,True])\n",
    "print(precision_test)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_proba = model_nb.predict_proba(X_train)\n",
    "y_test_proba = model_nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Web App\n",
    "- putting the functions together for the ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_input = \"The seller was very mean and rude. I will definitely not buy anything from them again, even the though product was very high quality.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2Vmodel = joblib.load(open('W2Vmodel.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = joblib.load(open('model_nb_200sellers.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_mike = ['bracelets','necklace','earrings',\n",
    "                 'bracelet','ring','daughter','mother',\n",
    "                 'anklet','cindy','gift','gorgeous',\n",
    "                 'item','product','color','colors',\n",
    "                 'lovely','perfect','cute','beautiful','great',\n",
    "                  'super','order','wonderful','work','awesome',\n",
    "                 'loved','lt','pretty','little','love','absolutely',\n",
    "                 'bunch','wear','friend','piece','amazing','happy',\n",
    "                  'fantastic','xx','summer','excellent','million',\n",
    "                 'michael','lisa','good','looks','highly','recommend',\n",
    "                 'price','leather','future','pleased','high','beads',\n",
    "                  'you','nice','size','looking',\n",
    "                 'design','buy','definitely','fast','quickly',\n",
    "                 'products','product','promptly','quickly','quick',\n",
    "                 'numbertoken','like','ordered','fit','purchase','bought',\n",
    "                  'christmas','beautifully','perfectly','purchased','husband',\n",
    "                  'ordering','adorable','way','small','better','wrist',\n",
    "                  'jewelry','wearing','day','wanted', 'easy', 'new','lot',\n",
    "                  'best','going', 'custom','sure', 'simple','right','person',\n",
    "                  'boyfriend','took', 'charm', 'sent', 'photo', 'week','use',\n",
    "                  'birthday','son','extra','bit','favorite', 'clasp','present',\n",
    "                  'nose','bag','sister','chain','mom', 'year', 'stone','wedding',\n",
    "                  'thought','delicate','feel','gave', 'different','dainty',\n",
    "                  'box','big','second','blanket','sweet','comfortable',\n",
    "                  'thing','extremely','baby','silver','want']\n",
    "\n",
    "uniqueness_pool = ['unique', 'creative', 'special',\n",
    "                   'inventive', 'innovative', 'handmade',\n",
    "                   'original', 'handcrafted',\n",
    "                   'creativity', 'crafted','made']\n",
    "\n",
    "humantouch_pool = ['helpful', 'communication', 'response',\n",
    "                   'respond', 'contact', 'friendly',\n",
    "                   'attentive', 'accommodating', 'courteous',\n",
    "                   'polite', 'respect', 'pleasant',\n",
    "                   'service','kind','personal',\n",
    "                   'personable','interaction','reply','answer',\n",
    "                   'honest','rude','unhelpful','unresponsive',\n",
    "                  'personalised','personalized']\n",
    "\n",
    "quality_pool = ['quality', 'condition', 'described',\n",
    "                'detailed', 'craftmanship', 'craftsmanship',\n",
    "                'detail', 'workmanship', 'material','broke','break',\n",
    "               'fell','apart']\n",
    "\n",
    "## for positive words in the word pools, also include synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = create_stopwords(stopwords_mike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_input_df = pd.DataFrame({'review_text' : [app_input]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting into sentences...\n",
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n",
      "removing stop words...\n",
      "tokenizing...\n"
     ]
    }
   ],
   "source": [
    "input_data_for_topic = preprocessing(app_input_df,\n",
    "                                     split_sentences = True,\n",
    "                                     replace_numbers = True,\n",
    "                                     tokenize = True,\n",
    "                                     lemmatize = True,\n",
    "                                     remove_stops = True,\n",
    "                                     embed = False,\n",
    "                                     split_on = \"[\\.!,]+[ ]*\",\n",
    "                                     stopwords = custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The seller was very mean and rude</td>\n",
       "      <td>[seller, mean, rude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will definitely not buy anything from them a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even the though product was very high quality</td>\n",
       "      <td>[quality]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text            clean_text\n",
       "0                  The seller was very mean and rude  [seller, mean, rude]\n",
       "1  I will definitely not buy anything from them a...                    []\n",
       "2      even the though product was very high quality             [quality]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_for_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = input_data_for_topic['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_sims = [similarity_test(r, uniqueness_pool, W2Vmodel.wv) for r in sentences]\n",
    "humantouch_sims = [similarity_test(r, humantouch_pool, W2Vmodel.wv) for r in sentences]\n",
    "quality_sims = [similarity_test(r, quality_pool, W2Vmodel.wv) for r in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_conts = [topic_contribution(sim) for sim in uniqueness_sims]\n",
    "humantouch_conts = [topic_contribution(sim) for sim in humantouch_sims]\n",
    "quality_conts = [topic_contribution(sim) for sim in quality_sims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting into sentences...\n",
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n"
     ]
    }
   ],
   "source": [
    "## process the data for the sentiment model\n",
    "data_for_sentiment = preprocessing(app_input_df,\n",
    "                                   split_sentences = True,\n",
    "                                   replace_numbers = True,\n",
    "                                   tokenize = False,\n",
    "                                   lemmatize = True,\n",
    "                                   remove_stops = False,\n",
    "                                   split_on = \"[\\.!,]+[ ]*\",\n",
    "                                   embed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     the seller wa very mean and rude\n",
       "1    i will definitely not buy anything from them a...\n",
       "2         even the though product wa very high quality\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_sentiment['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = model_nb.predict_proba(data_for_sentiment['clean_text'])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = sentiments * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74145284, -0.48133702,  0.37105345])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Score Using Topic Classification and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_score = np.sum(np.multiply(sentiments,uniqueness_conts))\n",
    "humantouch_score = np.sum(np.multiply(sentiments,humantouch_conts))\n",
    "quality_score = np.sum(np.multiply(sentiments,quality_conts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The seller was very mean and rude. I will definitely not buy anything from them again, even the though product was very high quality.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74145284, -0.48133702,  0.37105345])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.007427609650846983, -0.7340317757445779, 0.30432269643820997)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness_score,humantouch_score,quality_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.01, 0.0, 0.04], [1.0, 0.0, 0.02], [0.09, 0.0, 1.0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness_conts,humantouch_conts,quality_conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.0, 0.04]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness_conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74145284, -0.48133702,  0.37105345])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data_english.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n",
      "removing stop words...\n",
      "tokenizing...\n"
     ]
    }
   ],
   "source": [
    "training_topic = preprocessing(sample_data,\n",
    "                               split_sentences = False,\n",
    "                               replace_numbers = True,\n",
    "                               tokenize = True,\n",
    "                               lemmatize = True,\n",
    "                               remove_stops = True,\n",
    "                               embed = False,\n",
    "                               stopwords = custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>y</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13345</th>\n",
       "      <td>Friend loves it. Thanks!</td>\n",
       "      <td>True</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>[-0.056631826, 0.20809911, 0.05880275, 0.10198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27748</th>\n",
       "      <td>Perfect item once again. A special gift for my...</td>\n",
       "      <td>True</td>\n",
       "      <td>[special, hubby, thanks, x]</td>\n",
       "      <td>[0.049947638, 0.15428157, -0.053624954, -0.065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33198</th>\n",
       "      <td>This is part of a larger order; and like the p...</td>\n",
       "      <td>True</td>\n",
       "      <td>[larger, previous, posting, priced, , service]</td>\n",
       "      <td>[0.09703035, 0.21476015, -0.10617811, 0.110089...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18918</th>\n",
       "      <td>I love this so much I want another color.  It ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[, exactly, picture, expected, , ok, shipping,...</td>\n",
       "      <td>[0.08016536, 0.19877307, 0.027579254, 0.058497...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10494</th>\n",
       "      <td>Great shop to buy snap charms</td>\n",
       "      <td>True</td>\n",
       "      <td>[shop, snap]</td>\n",
       "      <td>[0.26379707, 0.23107846, -0.041005164, 0.14764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_text     y  \\\n",
       "13345                           Friend loves it. Thanks!  True   \n",
       "27748  Perfect item once again. A special gift for my...  True   \n",
       "33198  This is part of a larger order; and like the p...  True   \n",
       "18918  I love this so much I want another color.  It ...  True   \n",
       "10494                      Great shop to buy snap charms  True   \n",
       "\n",
       "                                              clean_text  \\\n",
       "13345                                           [thanks]   \n",
       "27748                        [special, hubby, thanks, x]   \n",
       "33198     [larger, previous, posting, priced, , service]   \n",
       "18918  [, exactly, picture, expected, , ok, shipping,...   \n",
       "10494                                       [shop, snap]   \n",
       "\n",
       "                                              embeddings  \n",
       "13345  [-0.056631826, 0.20809911, 0.05880275, 0.10198...  \n",
       "27748  [0.049947638, 0.15428157, -0.053624954, -0.065...  \n",
       "33198  [0.09703035, 0.21476015, -0.10617811, 0.110089...  \n",
       "18918  [0.08016536, 0.19877307, 0.027579254, 0.058497...  \n",
       "10494  [0.26379707, 0.23107846, -0.041005164, 0.14764...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = training_topic['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_sims = [similarity_test(r, uniqueness_pool, W2Vmodel.wv) for r in sentences]\n",
    "humantouch_sims = [similarity_test(r, humantouch_pool, W2Vmodel.wv) for r in sentences]\n",
    "quality_sims = [similarity_test(r, quality_pool, W2Vmodel.wv) for r in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_conts = [topic_contribution(sim) for sim in uniqueness_sims]\n",
    "humantouch_conts = [topic_contribution(sim) for sim in humantouch_sims]\n",
    "quality_conts = [topic_contribution(sim) for sim in quality_sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing accents...\n",
      "converting to lowercase...\n",
      "expanding contractions...\n",
      "replacing numbers...\n",
      "alphanumerizing...\n",
      "lemmatizing...\n"
     ]
    }
   ],
   "source": [
    "training_sentiment = preprocessing(sample_data,\n",
    "                                   split_sentences = False,\n",
    "                                   replace_numbers = True,\n",
    "                                   tokenize = False,\n",
    "                                   lemmatize = True,\n",
    "                                   remove_stops = False,\n",
    "                                   embed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = model_nb.predict_proba(training_sentiment['clean_text'])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = sentiments * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_scores = np.multiply(sentiments, uniqueness_conts)\n",
    "humantouch_score = np.multiply(sentiments, humantouch_conts)\n",
    "quality_score = np.multiply(sentiments, quality_conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentiment = pd.DataFrame({'uniqueness' : uniqueness_scores,\n",
    "                                  'humantouch' : humantouch_score,\n",
    "                                  'quality' : quality_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueness</th>\n",
       "      <th>humantouch</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.954465</td>\n",
       "      <td>0.229072</td>\n",
       "      <td>0.066813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.288799</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.213161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>0.037318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uniqueness  humantouch   quality\n",
       "0    0.000000    0.000000  0.031130\n",
       "1    0.954465    0.229072  0.066813\n",
       "2    0.288799    0.687616  0.213161\n",
       "3    0.000000    0.037318  0.037318\n",
       "4    0.000000    0.009540  0.000000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentiment.to_csv('training_sentiment.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentiment = pd.read_csv('training_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function seaborn.rcmod.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAADQCAYAAACA0oJxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACbFJREFUeJzt3WmsbecYB/D/Q81FU1OQ0KiZGNIb89yEfpAQSouq4oMhJCoiJNJQiTRIfCBiirSC6BA+aGhL5Yo0oYpOqhpBgi9cc01xeX3Y70135d5zznOHs8/p+f2Snfuetfda79rJfff+r2e9a+0aYwQAYKNut+odAAC2F+EBAGgRHgCAFuEBAGgRHgCAFuEBAGgRHgCAFuEBAGgRHgCAlqNW0elJJ500LrnkklV0DQAcWG3kRSupPOzZs2cV3QIAh4HTFgBAi/AAALQIDwBAi/AAALQIDwBAi/AAALQIDwBAy0puEgVsPy84+/x1X3PpWadswp4Aq6byAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQIvwAAC0CA8AQMtRq94BYOd4wdnnr/n8pWedskl7AhwKlQcAoEXlAThs1qssALcNKg8AQIvwAAC0CA8AQIvwAAC0CA8AQIurLYDbFPeSgCNP5QEAaBEeAIAWpy2AJG7wBGycygMA0KLyAOwoJlTCoVN5AABahAcAoEV4AABazHkAtoyNXPFxpOckbIV9gK1O5QEAaBEeAIAW4QEAaDHnAdhW3AkTVk/lAQBoER4AgBbhAQBoER4AgBbhAQBoER4AgBbhAQBoER4AgBbhAQBoER4AgBbhAQBoER4AgBbhAQBo8auaAE3r/bLnpWedskl7Aquh8gAAtAgPAECL8AAAtAgPAECL8AAAtAgPAECLSzVhG1jv0sDE5YHA5lF5AABaVB5gh9hI9QLYmrZa9VHlAQBoER4AgBbhAQBoMecBbiPMaQA2i8oDANAiPAAALcIDANBizgPAYXY45p+4YyhbmcoDANCi8gCwBa1XvVCZYJVUHgCAFpUH2ASOImH72ozxu93u06LyAAC0qDzAFrDdjjpgOznSlYOt9ouXm0HlAQBoUXlgpRwRwME5HGPHXJwFlb8+lQcAoEXlAQ6RoxZ2qq1wFcJOqY5sNSoPAECLygM7nsoBt0X+X3MkqTwAAC01xtj8Tqv+muSnm94xcDDunWTPqncC2JBDHa97xhgnrfeiVZ22+OkYY9eK+gYaquoq4xW2h80ar05bAAAtwgMA0LKq8PCpFfUL9BmvsH1synhdyYRJAGD7ctoCAGhZeXioqmOq6s1Lfz+gqi5a5T4Bt1ZVx1XVKw9y3ZsP9/4At1ZVb6yq02f7jKp6wNJzn6mqRx/W/lZ92qKqjkty8RjjsSvdEeCAquo5Sd4xxnjhfp47aoyxd411bx5jHH0k9w+4RVXtzmK8XnWk+li38jCPOH5SVZ+uqh9X1WVVdZeqOr6qLqmqH1TVd6rqkfP1x1fVd6vq+1V19r6jjqo6uqour6ofVtV1VfWi2cU5SY6vqqur6kOzv+vnOt+rqscs7cvuqjqhqu5WVZ+dffxoaVvAkoMYv+dW1clL6++rGpyT5JlznJ45j2wurKqvJrlsjfENrGOO0xur6ryquraqLqqqu1bVifM77rr5nXen+fpzquqG+doPz2Xvrap3zPG7K8kX5ni9y/zu3FVVb6qqDy71e0ZVfXS2T6uqK+c6n6yq26+502OMNR9JjkuyN8kT5t8XJDktyeVJHjaXPTnJt2b74iSvmO03Jrl5to9Kco/ZvneSnyWpuf3r/6+/62f7zCTvm+37J7lptj+Q5LTZPibJTUnutt578fDYaY+DGL/nJjl5af194/c5WVQI9y0/I8mvkxw7/97v+F7ehoeHx/4fc5yOJE+ff382yXuS/CrJw+eyzyV5W5Jjs7hD877xdcz8971ZVBuSZHeSXUvb351FoLhPkp8tLf96kmckeVSSrya5w1z+8SSnr7XPG53z8IsxxtWz/YP5Rp+W5MKqujrJJ7P4ck+Spya5cLa/uLSNSvKBqro2yTeTPDDJ/dbp94IkL5vtly9t9/lJ3jX73p3kzkketMH3AjtNZ/x2fGOM8YfZPpjxDdziV2OMK2b780lOzGLs3jSXnZfkWUn+kuSfST5TVS9J8veNdjDG+F2Sn1fVU6rqXkkekeSK2dcJSb4/PxNOTPKQtba10dtT/2up/Z8sPhT+NMZ4wkZ3Osmrskg9J4wx/l1Vv8ziS/+Axhi/qarfV9XjkpyS5A3zqUry0jGG38eA9XXG797M05lVVUnuuMZ2/7bUbo9v4FY2NAFxjLG3qp6UxRf8qUnekuR5jX7Oz+Jg/MYkXxljjDnWzxtjvHujGznYqy3+kuQXVfWyZPEhU1WPn899N8lLZ/vUpXXumeS384PluUkePJf/Ncnd1+jrS0nemeSeY4zr5rJLk7x1vuFU1RMP8n3ATrTW+P1lFkcgSfKiJHeY7fXG6YHGN7AxD6qqp872K7Ko4B1XVQ+dy16d5NtVdXQW34dfy+I0xv4OAtYar19O8uLZx77fbb88yclVdd8kqapjq2rNMXwol2q+Ksnrq+qaJD/O4oMmWbyZt1fVlVmUQv88l38hya6qumque2OSjDF+n+SKqrq+qj60n34uyiKEXLC07P1ZfKhdOydXvv8Q3gfsRAcav59O8uw5fp+cW6oL1ybZW1XXVNWZ+9nefsc3sGE/SfKaeerv2CQfSfLaLE4vXpfkv0k+kUUouHi+7ttZzA38f+cm+cS+CZPLT4wx/pjkhiQPHmNcOZfdkMUci8vmdr+RdU5lHvZLNavqrkn+MUshp2YxedLMawDYj9qGtyw4Ej/JfUKSj81TCn9K8roj0AcAsCIrv0kUALC9rPz21ADA9iI8AAAtwgMA0CI8AAAtwgMA0CI8AAAt/wNAAPussnV87wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 540x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probdata = pd.Series(training_sentiment.uniqueness, name = \"Sentiment\")\n",
    "plotdata = probdata[probdata != 0]\n",
    "g = sns.FacetGrid(plotdata, height = 3, aspect = 2.5)\n",
    "g = plt.hist(plotdata, bins=41, color='#4681a8')\n",
    "y, x, _ = g\n",
    "plt.xticks(np.array([-1,0,1]),['negative','neutral','positive'])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f013e6ce518>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probdata = pd.Series(training_sentiment.humantouch, name = \"Sentiment\")\n",
    "plotdata = probdata[probdata != 0]\n",
    "g = sns.FacetGrid(plotdata, height = 3, aspect = 2.5)\n",
    "g = plt.hist(plotdata, bins=41, color='#4681a8')\n",
    "y, x, _ = g\n",
    "plt.xticks(np.array([-1,0,1]),['negative','neutral','positive'])\n",
    "plt.yticks([])\n",
    "X_plot = np.array([0.4,0.4]); Y_plot = np.array([0,y.max()])\n",
    "plt.plot(X_plot, Y_plot, color='r', linewidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probdata = pd.Series(training_sentiment.quality, name = \"Sentiment\")\n",
    "plotdata = probdata[probdata != 0]\n",
    "g = sns.FacetGrid(plotdata, height = 3, aspect = 2.5)\n",
    "g = plt.hist(plotdata, bins=41, color='#4681a8')\n",
    "y, x, _ = g\n",
    "plt.xticks(np.array([-1,0,1]),['negative','neutral','positive'])\n",
    "plt.yticks([])\n",
    "plt.rc('xtick', labelsize=14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
